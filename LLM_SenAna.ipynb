{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sodjs\\RL\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig, Trainer\n",
    "from transformers import TrainingArguments\n",
    "import time\n",
    "import evaluate\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 12:30:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Nature #Park</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2023-01-15 08:45:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Traffic #Morning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "\n",
       "                                                Text    Sentiment  \\\n",
       "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
       "1   Traffic was terrible this morning.           ...   Negative     \n",
       "\n",
       "             Timestamp            User    Platform  \\\n",
       "0  2023-01-15 12:30:00   User123         Twitter     \n",
       "1  2023-01-15 08:45:00   CommuterX       Twitter     \n",
       "\n",
       "                                     Hashtags  Retweets  Likes     Country  \\\n",
       "0   #Nature #Park                                  15.0   30.0   USA         \n",
       "1   #Traffic #Morning                               5.0   10.0   Canada      \n",
       "\n",
       "   Year  Month  Day  Hour  \n",
       "0  2023      1   15    12  \n",
       "1  2023      1   15     8  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/sodjs/RL/data/sentimentdataset.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'Hashtags', 'Day', 'Hour'])\n",
    "df1['Sentiment'] = df1['Sentiment'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df1, test_size=0.1, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['Text', 'Sentiment']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['Text', 'Sentiment']])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (658, 2), 'test': (74, 2)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['Text', 'Sentiment'], 'test': ['Text', 'Sentiment']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Attended a classical music concert, feeling the timeless melodies resonate. Music transcends generations. #ClassicalMusic #TimelessMelodies \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Sentiment: \n",
      "joy\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      " Dancing through life with the exuberance of a carefree spirit, embracing joy and zest at every turn. \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Sentiment: \n",
      "zest\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = [20, 50]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example):\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i+1)\n",
    "    print(dash_line)\n",
    "    print(dataset['test'][index]['Text'])\n",
    "    print(dash_line)\n",
    "    print('Sentiment: ')\n",
    "    print(dataset['test'][index]['Sentiment'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sodjs\\RL\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable model params: 247577856\n",
      "All model params: 247577856\n",
      "100.0% of trainable params.\n"
     ]
    }
   ],
   "source": [
    "def print_no_trainable_param(model):\n",
    "    trainable_param = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_param += param.numel()\n",
    "    return f\"Number of trainable model params: {trainable_param}\\nAll model params: {all_param}\\n{100*trainable_param/all_param}% of trainable params.\"\n",
    "print(print_no_trainable_param(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Prompt Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Attended a classical music concert, feeling the timeless melodies resonate. Music transcends generations. #ClassicalMusic #TimelessMelodies Emotion: \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE GUIDED SENTIMENT:\n",
      "joy\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - No Prompt Engineering:\n",
      "The music transcends generations\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Dancing through life with the exuberance of a carefree spirit, embracing joy and zest at every turn. Emotion: \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE GUIDED SENTIMENT:\n",
      "zest\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - No Prompt Engineering:\n",
      "Dancing through life with the exuberance of a carefree spirit, embracing joy and zest at every turn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example):\n",
    "    text = dataset['test'][index]['Text'] \\\n",
    "    + \"Emotion: \"\n",
    "    sentiment = dataset['test'][index]['Sentiment']\n",
    "\n",
    "    input = tokenizer(text, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        original_model.generate(\n",
    "            input['input_ids'],\n",
    "            max_new_tokens=30\n",
    "        )[0], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i + 1)\n",
    "    print(dash_line)\n",
    "    print(f\"INPUT PROMPT:\\n{text}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE GUIDED SENTIMENT:\\n{sentiment}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION - No Prompt Engineering:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_shot_prompt(example_full, example_index):\n",
    "    prompt = ''\n",
    "    for index in example_full:\n",
    "        text = dataset['test'][index]['Text']\n",
    "        sentiment = dataset['test'][index]['Sentiment']\n",
    "\n",
    "        prompt += f\"\"\"\n",
    "Post:\n",
    "\n",
    "{text}\n",
    "\n",
    "What is the sentiment from the text?\n",
    "{sentiment}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "        text = dataset['test'][example_index]['Text']\n",
    "\n",
    "        prompt += f\"\"\"\n",
    "Post:\n",
    "\n",
    "{text}\n",
    "\n",
    "What is the sentiment from the text?\n",
    "\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post:\n",
      "\n",
      "Despite meticulous training, the swimmer faces disappointment as a split-second miscalculation costs them the lead in a crucial race. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "miscalculation\n",
      "\n",
      "\n",
      "\n",
      "Post:\n",
      "\n",
      " Dancing through life with the exuberance of a carefree spirit, embracing joy and zest at every turn. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_full = [5]\n",
    "example_index = 50\n",
    "\n",
    "one_shot = n_shot_prompt(example_full, example_index)\n",
    "print(one_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE GUIDED Sentiment:\n",
      "zest\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - One-shot:\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "Sentiment = dataset['test'][example_index]['Sentiment']\n",
    "\n",
    "input = tokenizer(one_shot, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        input['input_ids'],\n",
    "        max_new_tokens=50\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f\"BASELINE GUIDED Sentiment:\\n{Sentiment}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"MODEL GENERATION - One-shot:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post:\n",
      "\n",
      " Eyes wide open in the night, fearful shadows dancing on the walls, the mind a prisoner of imagined horrors. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "fearful\n",
      "\n",
      "\n",
      "\n",
      "Post:\n",
      "\n",
      " Dancing through life with the exuberance of a carefree spirit, embracing joy and zest at every turn. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "\n",
      "Post:\n",
      "\n",
      "Despite meticulous training, the swimmer faces disappointment as a split-second miscalculation costs them the lead in a crucial race. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "miscalculation\n",
      "\n",
      "\n",
      "\n",
      "Post:\n",
      "\n",
      " Dancing through life with the exuberance of a carefree spirit, embracing joy and zest at every turn. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "\n",
      "Post:\n",
      "\n",
      "Sorrowful echoes, a symphony of pain played by the strings of loss. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "sorrow\n",
      "\n",
      "\n",
      "\n",
      "Post:\n",
      "\n",
      " Dancing through life with the exuberance of a carefree spirit, embracing joy and zest at every turn. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "\n",
      "Post:\n",
      "\n",
      " Heartfelt sadness after bidding farewell to a dear friend. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "sadness\n",
      "\n",
      "\n",
      "\n",
      "Post:\n",
      "\n",
      " Dancing through life with the exuberance of a carefree spirit, embracing joy and zest at every turn. \n",
      "\n",
      "What is the sentiment from the text?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_full = [2, 5, 10, 15]\n",
    "example_index = 50\n",
    "three_shot_prompt = n_shot_prompt(example_full, example_index)\n",
    "print(three_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE GUIDED Sentiment:\n",
      "zest\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - One-shot:\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "Sentiment = dataset['test'][example_index]['Sentiment']\n",
    "\n",
    "input = tokenizer(three_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        input['input_ids'],\n",
    "        max_new_tokens=50\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f\"BASELINE GUIDED Sentiment:\\n{Sentiment}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"MODEL GENERATION - One-shot:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sodjs\\RL\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE GUIDED SENTIMENT:\n",
      "zest\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - Three-shot:\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "generate_config = GenerationConfig(max_new_token=20, do_sample=True, temperature=0.1, top_k=1)\n",
    "\n",
    "input = tokenizer(three_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        input['input_ids'],\n",
    "        generation_config=generate_config\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f\"BASELINE GUIDED SENTIMENT:\\n{Sentiment}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"MODEL GENERATION - Three-shot:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'What is the Sentiment from the post?\\n\\n'\n",
    "    end_prompt = '\\n\\nSentiment: '\n",
    "    prompt = [start_prompt + post + end_prompt for post in example[\"Text\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"Sentiment\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/658 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 658/658 [00:00<00:00, 2292.74 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 2551.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['Text', 'Sentiment', 'input_ids', 'labels'], 'test': ['Text', 'Sentiment', 'input_ids', 'labels']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 658/658 [00:00<00:00, 2464.42 examples/s]\n",
      "Filter: 100%|██████████| 74/74 [00:00<00:00, 2466.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(['Text', 'Sentiment'])\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda example, index: index % 100==0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7, 2)\n",
      "Test: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {tokenized_dataset['train'].shape}\")\n",
    "print(f\"Test: {tokenized_dataset['test'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./LLM_SenAna-{str(int(time.time()))}'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sodjs\\RL\\venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [3:54:06<00:00, 14046.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 38.25, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 14046.6946, 'train_samples_per_second': 0.001, 'train_steps_per_second': 0.0, 'train_loss': 38.25, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=38.25, metrics={'train_runtime': 14046.6946, 'train_samples_per_second': 0.001, 'train_steps_per_second': 0.0, 'train_loss': 38.25, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./LLM_SenAna-checkpoint-local\\\\tokenizer_config.json',\n",
       " './LLM_SenAna-checkpoint-local\\\\special_tokens_map.json',\n",
       " './LLM_SenAna-checkpoint-local\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_model_path = \"./LLM_SenAna-checkpoint-local\"\n",
    "trainer.model.save_pretrained(instruct_model_path)\n",
    "tokenizer.save_pretrained(instruct_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./LLM_SenAna-checkpoint-local\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = dataset['test'][0:10]['Text']\n",
    "given_sentiment = dataset['test'][0:10]['Sentiment']\n",
    "\n",
    "original_model_sentiment = []\n",
    "instruct_model_sentiment = []\n",
    "\n",
    "for _, post in enumerate(posts):\n",
    "    prompt = f\"\"\"\n",
    "What is the sentiment from the post?\n",
    "\n",
    "{post}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_sentiment.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_sentiment.append(instruct_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(given_sentiment, original_model_sentiment, instruct_model_sentiment))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['given_sentiment', 'original_model_sentiment', 'instruct_model_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.22857142857142856, 'rouge2': 0.0, 'rougeL': 0.2, 'rougeLsum': 0.22857142857142856}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.2, 'rouge2': 0.0, 'rougeL': 0.2, 'rougeLsum': 0.2}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_sentiment,\n",
    "    references=given_sentiment[0:len(original_model_sentiment)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_sentiment,\n",
    "    references=given_sentiment[0:len(instruct_model_sentiment)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: -2.86%\n",
      "rouge2: 0.00%\n",
      "rougeL: 0.00%\n",
      "rougeLsum: -2.86%\n"
     ]
    }
   ],
   "source": [
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable model params: 3538944\n",
      "All model params: 251116800\n",
      "1.4092820552029972% of trainable params.\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model,\n",
    "                            lora_config)\n",
    "print(print_no_trainable_param(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-LLM_SenAna-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [2:11:01<00:00, 7861.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 38.0, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 7861.5008, 'train_samples_per_second': 0.001, 'train_steps_per_second': 0.0, 'train_loss': 38.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./peft_LLM_SenAna-checkpoint-local\\\\tokenizer_config.json',\n",
       " './peft_LLM_SenAna-checkpoint-local\\\\special_tokens_map.json',\n",
       " './peft_LLM_SenAna-checkpoint-local\\\\tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "peft_model_path = \"./peft_LLM_SenAna-checkpoint-local\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
    "                                       './peft_LLM_SenAna-checkpoint-local',\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.22857142857142856, 'rouge2': 0.0, 'rougeL': 0.2, 'rougeLsum': 0.22857142857142856}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.2, 'rouge2': 0.0, 'rougeL': 0.2, 'rougeLsum': 0.2}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.5, 'rouge2': 0.0, 'rougeL': 0.5, 'rougeLsum': 0.5}\n"
     ]
    }
   ],
   "source": [
    "posts = dataset['test'][0:2]['Text']\n",
    "sentiments = dataset['test'][0:2]['Sentiment']\n",
    "original_model_sentiments = []\n",
    "instruct_model_sentiments = []\n",
    "peft_model_sentiments = []\n",
    "\n",
    "for idx, post in enumerate(posts):\n",
    "    prompt = f\"\"\"\n",
    "What is the sentiment of the post?\n",
    "\n",
    "{post}\n",
    "\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    human_baseline_text_output = sentiment[idx]\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_sentiments.append(original_model_text_output)\n",
    "    instruct_model_sentiments.append(instruct_model_text_output)\n",
    "    peft_model_sentiments.append(peft_model_text_output)\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_sentiments,\n",
    "    references=sentiments[0:len(peft_model_sentiments)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
